{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjdNCAPDdI2I"
   },
   "source": [
    "BUILDING A CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nn6Ix3JKdM-G"
   },
   "source": [
    "IMPORT THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "69XvTpIudO8O"
   },
   "outputs": [],
   "source": [
    "import  tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # for image data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "CbTEMtK1FsSL",
    "outputId": "58a473b8-179e-40e4-c1a4-bbb79a06f4dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSlzw_nco6Q0"
   },
   "source": [
    "DATA PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SStebvk6o8cT"
   },
   "source": [
    "PART 1- PREPROCESSING THE TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bTgZAi6_pDMi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True) #image augmentation tool\n",
    "training_set= train_datagen.flow_from_directory(\n",
    "        'D:\\\\Machine_learning\\\\CNN_totalset\\\\Section 40 - Convolutional Neural Networks (CNN)\\\\dataset\\\\training_set',  # this is the target directory\n",
    "        target_size=(64, 64),  # all images will be resized to 150x150\n",
    "        batch_size=32,\n",
    "        class_mode='binary') # choose class_mode based on outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEmttrFSD9AO"
   },
   "source": [
    "PREPROCESSING TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BpREq7YGD_Yl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255) #we will just apply rescaling on this\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'D:\\\\Machine_learning\\\\CNN_totalset\\\\Section 40 - Convolutional Neural Networks (CNN)\\\\dataset\\\\test_set', #target directory\n",
    "        target_size=(64, 64),# resizing the images\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XH99kvxDGEJT"
   },
   "source": [
    "PART2 - BUILDING THE CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BsdhnYZGPLc"
   },
   "source": [
    "INTIALIZING THE CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YeJaWicxGSC1"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDP1CFAJGk8S"
   },
   "source": [
    "CONVOLUTION OPERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WvQgNBftGqMG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reddy\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation=\"relu\",input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsyM3qvZMXno"
   },
   "source": [
    "MAX POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Y49KzR_HMZAr"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9iVXB6SMsym"
   },
   "source": [
    "ADDING SECOND CONVOLUTIONAL LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VmI4cfp7MwyQ"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation=\"relu\"))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2)) #here we are adding another convolutional layer for more accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwcD2-mFNUKF"
   },
   "source": [
    "FLATTENING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6gY-AiTUNWK1"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten()) #adding flattenclass ,here we don't need to mention any parameters,it is automatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtOAnXiYOkj3"
   },
   "source": [
    "FULL CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JbsjOWwtOn2G",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnrAkZ0bPhP1"
   },
   "source": [
    "ADDING THE OUTPUT LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hy7iIKFsPukL"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaRDSXUNN_G8"
   },
   "source": [
    "PART3 - TRAINING THE CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sxcoLa0ODDR"
   },
   "source": [
    "COMPILING THE CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QDMom5WBOZZV"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8PAN9JbOUSS"
   },
   "source": [
    "TRAINING THE CNN USING TRAINING TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hAR9U_cpzlQu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reddy\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 389ms/step - accuracy: 0.5669 - loss: 0.6843 - val_accuracy: 0.6795 - val_loss: 0.6152\n",
      "Epoch 2/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 124ms/step - accuracy: 0.6774 - loss: 0.6055 - val_accuracy: 0.6250 - val_loss: 0.6554\n",
      "Epoch 3/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 123ms/step - accuracy: 0.7017 - loss: 0.5729 - val_accuracy: 0.7100 - val_loss: 0.5547\n",
      "Epoch 4/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 123ms/step - accuracy: 0.7330 - loss: 0.5398 - val_accuracy: 0.7495 - val_loss: 0.5192\n",
      "Epoch 5/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 123ms/step - accuracy: 0.7521 - loss: 0.5094 - val_accuracy: 0.7160 - val_loss: 0.5831\n",
      "Epoch 6/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 130ms/step - accuracy: 0.7579 - loss: 0.4926 - val_accuracy: 0.7745 - val_loss: 0.4805\n",
      "Epoch 7/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 128ms/step - accuracy: 0.7890 - loss: 0.4525 - val_accuracy: 0.7550 - val_loss: 0.5072\n",
      "Epoch 8/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 123ms/step - accuracy: 0.7866 - loss: 0.4477 - val_accuracy: 0.7820 - val_loss: 0.4586\n",
      "Epoch 9/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 125ms/step - accuracy: 0.7983 - loss: 0.4286 - val_accuracy: 0.7425 - val_loss: 0.5446\n",
      "Epoch 10/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 126ms/step - accuracy: 0.8127 - loss: 0.4126 - val_accuracy: 0.7675 - val_loss: 0.4956\n",
      "Epoch 11/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 130ms/step - accuracy: 0.8065 - loss: 0.4182 - val_accuracy: 0.7715 - val_loss: 0.4839\n",
      "Epoch 12/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 129ms/step - accuracy: 0.8118 - loss: 0.4102 - val_accuracy: 0.7965 - val_loss: 0.4408\n",
      "Epoch 13/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 120ms/step - accuracy: 0.8169 - loss: 0.3935 - val_accuracy: 0.7875 - val_loss: 0.4513\n",
      "Epoch 14/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 121ms/step - accuracy: 0.8178 - loss: 0.3917 - val_accuracy: 0.7975 - val_loss: 0.4663\n",
      "Epoch 15/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 118ms/step - accuracy: 0.8239 - loss: 0.3676 - val_accuracy: 0.8095 - val_loss: 0.4312\n",
      "Epoch 16/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 118ms/step - accuracy: 0.8363 - loss: 0.3654 - val_accuracy: 0.8065 - val_loss: 0.4247\n",
      "Epoch 17/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 119ms/step - accuracy: 0.8482 - loss: 0.3473 - val_accuracy: 0.8115 - val_loss: 0.4347\n",
      "Epoch 18/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 120ms/step - accuracy: 0.8476 - loss: 0.3406 - val_accuracy: 0.8040 - val_loss: 0.4376\n",
      "Epoch 19/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 121ms/step - accuracy: 0.8423 - loss: 0.3503 - val_accuracy: 0.8225 - val_loss: 0.4418\n",
      "Epoch 20/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 120ms/step - accuracy: 0.8637 - loss: 0.3236 - val_accuracy: 0.8145 - val_loss: 0.4340\n",
      "Epoch 21/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 120ms/step - accuracy: 0.8650 - loss: 0.3189 - val_accuracy: 0.7975 - val_loss: 0.4719\n",
      "Epoch 22/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 126ms/step - accuracy: 0.8655 - loss: 0.3075 - val_accuracy: 0.8030 - val_loss: 0.4796\n",
      "Epoch 23/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 121ms/step - accuracy: 0.8709 - loss: 0.2966 - val_accuracy: 0.8110 - val_loss: 0.4857\n",
      "Epoch 24/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 121ms/step - accuracy: 0.8691 - loss: 0.2900 - val_accuracy: 0.8095 - val_loss: 0.4554\n",
      "Epoch 25/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 121ms/step - accuracy: 0.8769 - loss: 0.2796 - val_accuracy: 0.8100 - val_loss: 0.4654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19c34bf40d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set , epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdpDeVSnzwFo"
   },
   "source": [
    "SINGLE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qHvSmAnP11Gd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_img = image.load_img(\"D:\\\\Machine_learning\\\\CNN_totalset\\\\Section 40 - Convolutional Neural Networks (CNN)\\\\dataset\\\\single_prediction\\\\cat_or_dog_2.jpg\",target_size=(64,64))\n",
    "test_img = image.img_to_array(test_img)\n",
    "test_img = np.expand_dims(test_img,axis=0) #here we are increasing dimension by adding the batch(first dimension so we mention it as axis=0)\n",
    "result = cnn.predict(test_img/255.0) #rescaling the image(normalizing)\n",
    "#here we don't know wheter cat is 0 or dog is 1,so get that info we call training_set.class_indices\n",
    "training_set.class_indices\n",
    "if(result[0][0]>0.5): #result[0][0] gives the prediction\n",
    "  prediction = \"dog\"\n",
    "else:\n",
    "  prediction = \"cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc1_cENEFg1R"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nM3IBEQy6KlW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
